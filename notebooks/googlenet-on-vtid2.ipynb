{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# **Library Imports**\n","Import required libraries for data handling, image manipulation, plotting, machine learning, and model evaluation."],"metadata":{"id":"dAED51_wfId7"}},{"cell_type":"code","source":["!pip install seaborn"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-19T01:21:00.739291Z","iopub.execute_input":"2023-08-19T01:21:00.740226Z","iopub.status.idle":"2023-08-19T01:21:06.884397Z","shell.execute_reply.started":"2023-08-19T01:21:00.740193Z","shell.execute_reply":"2023-08-19T01:21:06.883439Z"},"trusted":true,"id":"IHeI_8ySbRIE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import h5py\n","import cv2\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.optim as optim\n","import seaborn as sns\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import classification_report, roc_curve, confusion_matrix\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import LabelEncoder, label_binarize"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:21:06.886156Z","iopub.execute_input":"2023-08-19T01:21:06.886439Z","iopub.status.idle":"2023-08-19T01:21:33.022492Z","shell.execute_reply.started":"2023-08-19T01:21:06.886410Z","shell.execute_reply":"2023-08-19T01:21:33.021350Z"},"trusted":true,"id":"0MGsTZVgbRIK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Load Data**\n","Read the dataset (bitvehicle_dataset.h5) using h5py and print the total number of images and labels."],"metadata":{"id":"5ng3_gPCfMc9"}},{"cell_type":"code","source":["with h5py.File('/kaggle/input/vtid2-dataset/vtid2_dataset.h5', 'r') as file:\n","    images = file['images'][:]\n","    labels = file['labels'][:]\n","\n","print('Total number of images: ',len(images))\n","print('Total number of labels: ',len(labels))"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:21:33.023747Z","iopub.execute_input":"2023-08-19T01:21:33.024212Z","iopub.status.idle":"2023-08-19T01:21:46.532308Z","shell.execute_reply.started":"2023-08-19T01:21:33.024183Z","shell.execute_reply":"2023-08-19T01:21:46.531325Z"},"trusted":true,"id":"GEKpDeeqbRIO","outputId":"520a072e-df92-4ced-e685-1af2eddda017"},"execution_count":null,"outputs":[{"name":"stdout","text":"Total number of images:  4356\nTotal number of labels:  4356\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# **Data Exploration**\n","Print the number of samples for each unique class in the dataset."],"metadata":{"id":"u9r_1kNEfQIl"}},{"cell_type":"code","source":["unique_classes, class_counts = np.unique(labels, return_counts=True)\n","for class_label, count in zip(unique_classes, class_counts):\n","    print(f\"Class {class_label}: {count} samples\")"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:21:46.534507Z","iopub.execute_input":"2023-08-19T01:21:46.534783Z","iopub.status.idle":"2023-08-19T01:21:46.541990Z","shell.execute_reply.started":"2023-08-19T01:21:46.534758Z","shell.execute_reply":"2023-08-19T01:21:46.541118Z"},"trusted":true,"id":"2RKf7rP6bRIQ","outputId":"bca45a33-5b55-49ca-87ad-748c32a958cb"},"execution_count":null,"outputs":[{"name":"stdout","text":"Class b'hatchback': 606 samples\nClass b'other': 600 samples\nClass b'pickup': 1240 samples\nClass b'sedan': 1230 samples\nClass b'suv': 680 samples\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# **Image Resizing and Preprocessing**\n","Resize images to (224, 224) and preprocess them using ResNet50's preprocess_input function."],"metadata":{"id":"pq9aWeoJfX67"}},{"cell_type":"code","source":["resized_images = [Image.fromarray(image).resize((224, 224)) for image in images]\n","resized_images = np.array([preprocess_input(np.array(image)) for image in resized_images])"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:21:46.543201Z","iopub.execute_input":"2023-08-19T01:21:46.543578Z","iopub.status.idle":"2023-08-19T01:21:56.381669Z","shell.execute_reply.started":"2023-08-19T01:21:46.543552Z","shell.execute_reply":"2023-08-19T01:21:56.380800Z"},"trusted":true,"id":"FhJmvqWybRIR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Data Augmentation Functions**\n","Define functions for augmenting images (brightness adjustment, flipping, rotating, zooming, and shifting)."],"metadata":{"id":"bxd_fgzffhQ2"}},{"cell_type":"code","source":["def adjust_brightness(image, factor):\n","    image = image.astype(np.float32)\n","    augmented_image = image + factor\n","    augmented_image = np.clip(augmented_image, 0, 255)\n","    augmented_image = augmented_image.astype(np.uint8)\n","    return augmented_image\n","\n","def flip_image(image, flip_code):\n","    return cv2.flip(image, flip_code)\n","\n","def rotate_image(image, angle):\n","    rows, cols = image.shape[:2]\n","    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n","    return cv2.warpAffine(image, M, (cols, rows))\n","\n","def zoom_image(image, zoom_factor):\n","    rows, cols = image.shape[:2]\n","    M = cv2.getRotationMatrix2D((cols / 2, rows / 2), 0, zoom_factor)\n","    return cv2.warpAffine(image, M, (cols, rows))\n","\n","def shift_image(image, dx, dy):\n","    rows, cols = image.shape[:2]\n","    M = np.float32([[1, 0, dx], [0, 1, dy]])\n","    return cv2.warpAffine(image, M, (cols, rows))"],"metadata":{"id":"dK227bwzfqBo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Data Augmentation Application**\n","Augment images based on their class labels and collect the augmented images and labels."],"metadata":{"id":"qqlv2qFxfrO3"}},{"cell_type":"code","source":["augmented_images = []\n","augmented_labels = []\n","\n","for img, label in zip(resized_images, labels):\n","    augmented_img_brightness = adjust_brightness(img, 50)\n","\n","    augmented_img_flip_horizontal = flip_image(img, 1)\n","    augmented_img_flip_vertical = flip_image(img, 0)\n","\n","    augmented_img_rotate = rotate_image(img, 30)\n","\n","    augmented_img_zoom = zoom_image(img, 1.2)\n","\n","    augmented_img_shift = shift_image(img, 20, 20)\n","\n","    augmented_images.extend([\n","        img,\n","        augmented_img_brightness,\n","        augmented_img_flip_horizontal,\n","        augmented_img_flip_vertical,\n","        augmented_img_rotate,\n","        augmented_img_zoom,\n","        augmented_img_shift\n","    ])\n","\n","    augmented_labels.extend([label] * 7)\n","\n","print('Total number of augmented images: ',len(augmented_images))\n","print('Total number of augmented labels: ',len(augmented_labels))"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:21:56.382971Z","iopub.execute_input":"2023-08-19T01:21:56.383292Z","iopub.status.idle":"2023-08-19T01:22:06.311165Z","shell.execute_reply.started":"2023-08-19T01:21:56.383264Z","shell.execute_reply":"2023-08-19T01:22:06.310279Z"},"trusted":true,"id":"hvpNs6FCbRIS","outputId":"6cc1e6c0-7f05-4a27-91e3-261f731f4104"},"execution_count":null,"outputs":[{"name":"stdout","text":"Total number of augmented images:  30492\nTotal number of augmented labels:  30492\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# **Augmentation Summary**\n","Print the number of samples for each class."],"metadata":{"id":"-eva4stsfzRb"}},{"cell_type":"code","source":["unique_classes_aug, class_counts_aug = np.unique(augmented_labels, return_counts=True)\n","for class_label, count in zip(unique_classes_aug, class_counts_aug):\n","    print(f\"Class {class_label}: {count} samples\")"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:22:06.312262Z","iopub.execute_input":"2023-08-19T01:22:06.312559Z","iopub.status.idle":"2023-08-19T01:22:06.323744Z","shell.execute_reply.started":"2023-08-19T01:22:06.312534Z","shell.execute_reply":"2023-08-19T01:22:06.322855Z"},"trusted":true,"id":"y6rH6--wbRIT","outputId":"a7a813b2-e7c2-457b-a209-7d4553b7e742"},"execution_count":null,"outputs":[{"name":"stdout","text":"Class b'hatchback': 4242 samples\nClass b'other': 4200 samples\nClass b'pickup': 8680 samples\nClass b'sedan': 8610 samples\nClass b'suv': 4760 samples\n","output_type":"stream"}]},{"cell_type":"code","source":["num_classes = len(np.unique(augmented_labels))\n","print('Number of classes',num_classes)"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:22:06.324894Z","iopub.execute_input":"2023-08-19T01:22:06.325182Z","iopub.status.idle":"2023-08-19T01:22:06.341491Z","shell.execute_reply.started":"2023-08-19T01:22:06.325157Z","shell.execute_reply":"2023-08-19T01:22:06.340714Z"},"trusted":true,"id":"xGxEk2UFbRIU","outputId":"0b789696-3601-401b-d927-4aed77408bd2"},"execution_count":null,"outputs":[{"name":"stdout","text":"Number of classes 5\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# **Label Encoding and One-Hot Encoding**\n","Convert labels to numerical format and one-hot encode the labels."],"metadata":{"id":"rcIoH10Cf8l1"}},{"cell_type":"code","source":["label_encoder = LabelEncoder()\n","encoded_labels = label_encoder.fit_transform(augmented_labels)\n","label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}\n","print(label_mapping)\n","label = torch.tensor(encoded_labels)\n","labels = nn.functional.one_hot(label, num_classes=num_classes)\n","labels = labels.float()"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:22:06.342504Z","iopub.execute_input":"2023-08-19T01:22:06.342807Z","iopub.status.idle":"2023-08-19T01:22:06.364410Z","shell.execute_reply.started":"2023-08-19T01:22:06.342781Z","shell.execute_reply":"2023-08-19T01:22:06.363443Z"},"trusted":true,"id":"7hFOipm1bRIV","outputId":"2c80a592-da87-4198-954a-4df1843a9f32"},"execution_count":null,"outputs":[{"name":"stdout","text":"{0: b'hatchback', 1: b'other', 2: b'pickup', 3: b'sedan', 4: b'suv'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# **Custom dataset class named ImageDataset for PyTorch**"],"metadata":{"id":"Mg0wz7Fwesog"}},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","    def __init__(self, data, labels, transform=None):\n","        self.data = data\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        x = self.data[index]\n","        y = self.labels[index]\n","\n","        if self.transform:\n","            x = self.transform(x)\n","\n","        return x, y\n","transform = transforms.Compose([\n","\n","    transforms.ToPILImage(),\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","])\n","\n","\n","dataset = ImageDataset(np.array(augmented_images), labels, transform=transform)"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:22:06.367982Z","iopub.execute_input":"2023-08-19T01:22:06.368306Z","iopub.status.idle":"2023-08-19T01:22:07.712886Z","shell.execute_reply.started":"2023-08-19T01:22:06.368278Z","shell.execute_reply":"2023-08-19T01:22:07.711914Z"},"trusted":true,"id":"ROsZU9onbRIY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Model Building**\n","Setting Up a Pretrained GoogLeNet Model for Classification"],"metadata":{"id":"D_9dyIGme1UU"}},{"cell_type":"code","source":["model = models.googlenet(pretrained=True)\n","num_input_features = model.fc.in_features\n","\n","model.fc = nn.Linear(num_input_features, num_classes)\n","\n","use_cuda = torch.cuda.is_available()\n","\n","if use_cuda:\n","    model.cuda()\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:22:07.714114Z","iopub.execute_input":"2023-08-19T01:22:07.714445Z","iopub.status.idle":"2023-08-19T01:22:08.392476Z","shell.execute_reply.started":"2023-08-19T01:22:07.714417Z","shell.execute_reply":"2023-08-19T01:22:08.391511Z"},"trusted":true,"id":"XVpIdnohbRIY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **K-Fold Cross-Validation Setup**\n","Initialize K-Fold cross-validation (3 folds) and define variables to collect evaluation metrics.\n","\n","# **Model Training and Evaluation**\n","For each fold, train the model, make predictions, and evaluate the model's performance using accuracy, precision, recall, F1-score, and confusion matrix.\n","\n","# **Average Performance Metrics**\n","Calculate and print the average accuracy, weighted precision, recall, and F1-score across all folds."],"metadata":{"id":"_yQZRhK7gYSr"}},{"cell_type":"code","source":["learning_rate = 0.0001\n","epochs = 3\n","batch_size = 32\n","k = 3\n","\n","kf = KFold(n_splits=k, shuffle=True)\n","accuracy_values = []\n","weighted_precision_values = []\n","weighted_recall_values = []\n","weighted_f1_score_values = []\n","\n","\n","all_true_labels = []\n","all_pred_labels = []\n","confusion_matrices = []\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","for fold, (train_index, test_index) in enumerate(kf.split(dataset), 1):\n","    print(\"Fold:\", fold)\n","    train_sampler = torch.utils.data.SubsetRandomSampler(train_index)\n","    test_sampler = torch.utils.data.SubsetRandomSampler(test_index)\n","\n","    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n","    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        running_loss = 0.0\n","        for batch_idx, (inputs, labels) in enumerate(train_loader):\n","          inputs = inputs.to(device)\n","          labels = labels.to(device)\n","          outputs = model(inputs)\n","          loss = criterion(outputs, labels)\n","          optimizer.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","          running_loss += loss.item()\n","\n","          print(f\"\\rEpoch {epoch+1}/{epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\", end='')\n","\n","        print(f\"\\rEpoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n","\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","\n","    for batch_idx, (inputs, labels) in enumerate(test_loader):\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        outputs = model(inputs)\n","        predicted = F.softmax(outputs, dim=1)\n","\n","        y_true.extend(labels.cpu().tolist())\n","        y_pred.extend(predicted.cpu().tolist())\n","\n","    print(f\"\\rEvaluation: Batch {batch_idx+1}/{len(test_loader)}\", end='\\n')\n","\n","    y_true_labels = np.argmax(y_true, axis=1)\n","    y_pred_labels = np.argmax(y_pred, axis=1)\n","\n","    cm = confusion_matrix(y_true_labels, y_pred_labels)\n","    report = classification_report(y_true_labels, y_pred_labels, output_dict=True)\n","\n","    confusion_matrices.append(cm)\n","    accuracy_values.append(report['accuracy'])\n","    weighted_precision_values.append(report['weighted avg']['precision'])\n","    weighted_recall_values.append(report['weighted avg']['recall'])\n","    weighted_f1_score_values.append(report['weighted avg']['f1-score'])\n","\n","    all_true_labels.extend(y_true_labels)\n","    all_pred_labels.extend(y_pred)\n","\n","\n","avg_accuracy = np.mean(accuracy_values)\n","avg_weighted_precision = np.mean(weighted_precision_values)\n","avg_weighted_recall = np.mean(weighted_recall_values)\n","weighted_f1_score_values = np.mean(weighted_f1_score_values)\n","\n","print('Average accuracy:', avg_accuracy)\n","print('Average weighted precision:', avg_weighted_precision)\n","print('Average weighted recall:', avg_weighted_recall)\n","print('Average weighted f1_score:', weighted_f1_score_values)"],"metadata":{"execution":{"iopub.status.busy":"2023-08-19T01:22:08.393999Z","iopub.execute_input":"2023-08-19T01:22:08.394352Z","iopub.status.idle":"2023-08-19T02:13:07.218219Z","shell.execute_reply.started":"2023-08-19T01:22:08.394322Z","shell.execute_reply":"2023-08-19T02:13:07.216934Z"},"trusted":true,"id":"onxyoAKTbRIZ","outputId":"fdcce000-075f-4372-deea-7ffd16e8bd29"},"execution_count":null,"outputs":[{"name":"stdout","text":"Fold: 1\nEpoch 1/3, Loss: 0.19766, Loss: 0.0173\nEpoch 2/3, Loss: 0.01086, Loss: 0.2468\nEpoch 3/3, Loss: 0.01266, Loss: 0.0014\nEvaluation: Batch 318/318\nFold: 2\nEpoch 1/3, Loss: 0.00676, Loss: 0.0031\nEpoch 2/3, Loss: 0.00906, Loss: 0.0006\nEpoch 3/3, Loss: 0.00616, Loss: 0.0216\nEvaluation: Batch 318/318\nFold: 3\nEpoch 1/3, Loss: 0.00506, Loss: 0.0039\nEpoch 2/3, Loss: 0.00506, Loss: 0.0002\nEpoch 3/3, Loss: 0.00696, Loss: 0.0013\nEvaluation: Batch 318/318\nAverage accuracy: 0.9991473173291355\nAverage weighted precision: 0.9991486530034693\nAverage weighted recall: 0.9991473173291355\nAverage weighted f1_score: 0.999146552037773\n","output_type":"stream"}]}]}